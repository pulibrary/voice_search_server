## Voice Search Server

A websockets server that provides transcription of short spoken webm audio
via the [Whisper model](https://openai.com/index/whisper/).

The use case is for a user to speak a search query into their browser
and quickly get back an accurate text transcription of their query in the search box.

### Setup

1. Install rust
1. `brew install cmake pkgconf opus`

### Basic client for testing

1. Run: `ruby -run -e httpd . -p 7020`
1. In your browser, go to http://localhost:7020/test_client.html

### Creating your own recording

1. Run: `ruby -run -e httpd . -p 7020`
1. In Chrome, go to http://localhost:7020/quick_record.html
1. Start microphone
1. Start recording.  When prompted, put your recording in the test_data directory
1. Stop recording
1. Stop microphone

### Creating a webm recording from an existing recording (e.g. a librivox)

1. Open the recording in audacity
1. Clip/modify the recording as needed
1. File -> Export Audio
1. Format: Custom FFmpeg Export
1. Open custom FFmpeg format options
1. Format: webm
1. Codec: libopus

### Todo
* MPSC channel should close when done
* MPSC channel should send on each chunk, not at the end
* The in-browser tester does not work on firefox?
* Really refactor the transcription
* finish writing transcription tests (at least one mono and one stereo per language).
* persist the model, config, and tokenizer on disk.  Populate them on startup if not present.  Definitely populate them before running the test suite (https://stackoverflow.com/a/58006287 perhaps?). Add a way to refresh them (a binary?). -- maybe this is already happening?
* don't hardcode metal!
* ci
* extract configuration options into their own file, maybe do some tuning!

### Mermaid

```mermaid
sequenceDiagram
  actor Client
  Client->>Server: Websockets handshake
  Client->>Server: Send binary websockets message with ogg-encoded WebM audio
  Server->>Audio module: Send audio for processing
  Audio module->>Audio module: Convert audio to PCM samples
  Audio module->>Audio module: Convert PCM samples to MEL features
  Audio module->>Whisper: Send MEL features
  Whisper->>Server: Send full or partial transcription
  Server->>Client: Send plaintext websocket message with transcription
```
